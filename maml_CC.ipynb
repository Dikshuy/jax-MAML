{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "maml-CC.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFu7h6jh7JWPSc9GJokwiI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dikshuy/jax-MAML/blob/main/maml_CC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QHnG_gYsFlr"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "!pip install optax\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp              \n",
        "import optax                         \n",
        "import numpy as np                   \n",
        "import tensorflow_datasets as tfds    \n",
        "import tensorflow as tf\n",
        "from jax import grad\n",
        "from jax import vmap \n",
        "from functools import partial\n",
        "from jax import jit \n",
        "from jax.experimental import stax \n",
        "from jax.experimental.stax import Conv, Dense, MaxPool, Relu, Flatten, Softmax, LogSoftmax, AvgPool, BatchNorm\n",
        "import matplotlib.pyplot as plt \n",
        "from jax.experimental import optimizers\n",
        "from jax.tree_util import tree_multimap\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "import urllib3\n",
        "import numpy as onp\n",
        "import argparse\n",
        "from jax.experimental import optimizers\n",
        "from jax.tree_util import tree_multimap\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "\n",
        "meta_step_size = 0.001\n",
        "\n",
        "batch_size = 25\n",
        "\n",
        "meta_iters = 2000\n",
        "eval_iters = 5\n",
        "inner_iters = 4\n",
        "\n",
        "eval_interval = 1\n",
        "train_shots = 20\n",
        "shots = 5\n",
        "classes = 5\n",
        "\n",
        "class Dataset:\n",
        "    # This class will facilitate the creation of a few-shot dataset\n",
        "    # from the Omniglot dataset that can be sampled from quickly while also\n",
        "    # allowing to create new labels at the same time.\n",
        "    def __init__(self, training):\n",
        "        # Download the tfrecord files containing the omniglot data and convert to a\n",
        "        # dataset.\n",
        "        split = \"train\" if training else \"test\"\n",
        "        ds = tfds.load(\"omniglot\", split=split, as_supervised=True, shuffle_files=False)\n",
        "        # Iterate over the dataset to get each individual image and its class,\n",
        "        # and put that data into a dictionary.\n",
        "        self.data = {}\n",
        "\n",
        "        def extraction(image, label):\n",
        "            # This function will shrink the Omniglot images to the desired size,\n",
        "            # scale pixel values and convert the RGB image to grayscale\n",
        "            image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "            image = tf.image.rgb_to_grayscale(image)\n",
        "            image = tf.image.resize(image, [28, 28])\n",
        "            return image, label\n",
        "\n",
        "        for image, label in ds.map(extraction):\n",
        "            image = image.numpy()\n",
        "            label = str(label.numpy())\n",
        "            if label not in self.data:\n",
        "                self.data[label] = []\n",
        "            self.data[label].append(image)\n",
        "        self.labels = list(self.data.keys())\n",
        "\n",
        "\n",
        "    def get_mini_dataset(\n",
        "        self, shots = shots, num_classes = classes\n",
        "    ):\n",
        "        temp_labels = np.zeros(shape=(num_classes * shots))\n",
        "        temp_images = np.zeros(shape=(num_classes * shots, 28, 28, 1))\n",
        "        test_labels = np.zeros(shape=(num_classes))\n",
        "        test_images = np.zeros(shape=(num_classes, 28, 28, 1))\n",
        "\n",
        "        # Get a random subset of labels from the entire label set.\n",
        "        label_subset = random.choices(self.labels, k=num_classes)\n",
        "        for class_idx, class_obj in enumerate(label_subset):\n",
        "            # Use enumerated index value as a temporary label for mini-batch in\n",
        "            # few shot learning.\n",
        "            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n",
        "            # label to create the test dataset.\n",
        "            test_labels[class_idx] = class_idx\n",
        "            images_to_split = random.choices(\n",
        "                self.data[label_subset[class_idx]], k=shots + 1\n",
        "            )\n",
        "            test_images[class_idx] = images_to_split[-1]\n",
        "            temp_images[\n",
        "                class_idx * shots : (class_idx + 1) * shots\n",
        "            ] = images_to_split[:-1]\n",
        "        \n",
        "        temp_images, temp_labels = shuffle(temp_images.astype(np.float32), temp_labels.astype(np.int32))\n",
        "        \n",
        "        support_set = {'images': temp_images, 'labels': temp_labels}\n",
        "        query_set = {'images': test_images, 'labels': test_labels}\n",
        "        \n",
        "        return support_set, query_set\n",
        "\n",
        "urllib3.disable_warnings()  # Disable SSL warnings that may happen during download.\n",
        "\n",
        "losses = []\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"--inner-loops\", help=\"number of inner loops: [1,2,3,4,...]\", type=list, default=[1]\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--learning-rates\", help=\"learning rates: [0.01, 0.05, ...]\", type=list, default=[0.01]\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--train-steps\", help=\"number of training steps: 100\", type=int, default=100\n",
        ")\n",
        "\n",
        "args = parser.parse_args()\n",
        "learning_rates = args.learning_rates\n",
        "num_inner_loops = args.inner_loops\n",
        "TRAIN_STEPS = args.train_steps\n",
        "\n",
        "for lr in learning_rates:\n",
        "  for inner_loop in num_inner_loops:\n",
        "    maml_losses = []\n",
        "    for i in tqdm(range(TRAIN_STEPS)):\n",
        "      train_dataset = Dataset(training=True)\n",
        "      test_dataset = Dataset(training=False)\n",
        "\n",
        "      net_init, net_apply = stax.serial(\n",
        "          Conv(out_chan = 64, filter_shape = (3,3), strides = [1,1], padding = 'SAME'), BatchNorm(), Relu,\n",
        "          Conv(out_chan = 64, filter_shape = (3,3), strides = [1,1], padding = 'SAME'), BatchNorm(), Relu,\n",
        "          Conv(out_chan = 64, filter_shape = (3,3), strides = [1,1], padding = 'SAME'), BatchNorm(), Relu,\n",
        "          Conv(out_chan = 64, filter_shape = (3,3), strides = [1,1], padding = 'SAME'), BatchNorm(), Relu,\n",
        "          AvgPool((28, 28)),\n",
        "          Flatten,\n",
        "          Dense(classes),\n",
        "      )\n",
        "\n",
        "      in_shape = (-1, 28, 28, 1)\n",
        "      out_shape, net_params = net_init(rng, in_shape)\n",
        "\n",
        "      opt_init, opt_update, get_params = optimizers.adam(step_size=meta_step_size)\n",
        "      opt_state = opt_init(net_params)\n",
        "\n",
        "      @jit\n",
        "      def maml_loss_batch(params, x1_b, y1_b, x2_b, y2_b):\n",
        "          '''\n",
        "          input:\n",
        "          - params\n",
        "          - x1_b, y1_b, x2_b, y2_b: batches of sample task \n",
        "          output:\n",
        "          - combined loss of the batch\n",
        "          '''   \n",
        "          return  onp.mean(vmap(partial(maml_loss, params))(x1_b, y1_b, x2_b, y2_b))\n",
        "\n",
        "      @jit\n",
        "      def batch_step(i, opt_state, x1_b, y1_b, x2_b, y2_b):\n",
        "          p = get_params(opt_state)\n",
        "          g = grad(maml_loss_batch)(p, x1_b, y1_b, x2_b, y2_b)\n",
        "          l = maml_loss_batch(p, x1_b, y1_b, x2_b, y2_b)\n",
        "          return opt_update(i, g, opt_state), l\n",
        "\n",
        "\n",
        "      @jit\n",
        "      def loss(params, inputs, targets):\n",
        "          predictions = net_apply(params, inputs)\n",
        "          loss_ = jnp.mean(optax.softmax_cross_entropy(predictions, jax.nn.one_hot(targets, num_classes=classes)))\n",
        "          return loss_\n",
        "\n",
        "      @jit\n",
        "      def inner_update(params, inputs, outputs, alpha = lr):\n",
        "          '''\n",
        "          input:\n",
        "          - params: model's parameters\n",
        "          - inputs\n",
        "          - targets: true label\n",
        "          output\n",
        "          - updated parameters\n",
        "          '''\n",
        "          grads = grad(loss)(params, inputs, outputs)\n",
        "          grad_update_fn = lambda g, state: (state - alpha * g)\n",
        "          return tree_multimap(grad_update_fn, grads, params)\n",
        "\n",
        "      @jit\n",
        "      def maml_loss(params, support_img, support_lab, query_img, query_lab, num_inner_loops=inner_loop):\n",
        "          '''\n",
        "          input:\n",
        "          - params: model's parameters\n",
        "          - x1, y1: task's train set\n",
        "          - x2, y2: task's test set\n",
        "          output:\n",
        "          - Loss after update parameters 1 time on the test set.\n",
        "          '''\n",
        "          params_updated = params\n",
        "          for _ in range(num_inner_loops):\n",
        "              params_updated = inner_update(params_updated, support_img, support_lab)\n",
        "          total_loss = loss(params_updated, query_img, query_lab)\n",
        "          return total_loss\n",
        "\n",
        "      # get x_support, y_support, x_query, y_query batch\n",
        "      x_support_batch = []\n",
        "      y_support_batch = []\n",
        "      x_query_batch = []\n",
        "      y_query_batch = []\n",
        "      for j in range(batch_size):\n",
        "          support, query = train_dataset.get_mini_dataset()\n",
        "          x_support, y_support = support['images'], support['labels']\n",
        "          x_query, y_query = query['images'], query['labels']\n",
        "          x_support_batch.append(x_support)\n",
        "          y_support_batch.append(y_support)\n",
        "          x_query_batch.append(x_query)\n",
        "          y_query_batch.append(y_query)\n",
        "      x_support_batch = np.stack(x_support_batch)\n",
        "      y_support_batch = np.stack(y_support_batch)\n",
        "      x_query_batch = np.stack(x_query_batch)\n",
        "      y_query_batch = np.stack(y_query_batch)\n",
        "      opt_state, l = batch_step(i, opt_state, x_support_batch, y_support_batch, x_query_batch, y_query_batch)\n",
        "      maml_losses.append(l)\n",
        "\n",
        "    net_params = get_params(opt_state)\n",
        "    # plt.plot(np.arange(TRAIN_STEPS), np.array(maml_losses))\n",
        "    losses.append(maml_losses)\n",
        "\n",
        "for maml_losses in losses:\n",
        "  plt.plot(np.arange(TRAIN_STEPS), np.array(maml_losses))\n",
        "plt.show()"
      ]
    }
  ]
}