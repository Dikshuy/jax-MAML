{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "maml-CC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dikshuy/jax-MAML/blob/main/maml_CC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKwxqwBcZDFo",
        "outputId": "b0b2b377-918e-4723-a4a0-d2f96fd9c9d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.3-py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.14)\n",
            "Collecting chex>=0.0.4\n",
            "  Downloading chex-0.1.3-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 804 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from optax) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from optax) (1.2.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.14+cuda11.cudnn805)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from optax) (4.1.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.7)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (1.7.3)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (0.6.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.1.55->optax) (5.8.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.1.55->optax) (3.8.1)\n",
            "Installing collected packages: chex, optax\n",
            "Successfully installed chex-0.1.3 optax-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8QHnG_gYsFlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d284c64d-4f2a-4602-c89d-9e5d30d4a8f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/experimental/stax.py:30: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/experimental/optimizers.py:30: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead\n",
            "  FutureWarning)\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py:201: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
            "  'instead as a drop-in replacement.', FutureWarning)\n",
            "100%|██████████| 2/2 [02:43<00:00, 81.87s/it]\n",
            "100%|██████████| 2/2 [02:33<00:00, 76.89s/it]\n",
            "100%|██████████| 2/2 [02:02<00:00, 61.48s/it]\n",
            "100%|██████████| 2/2 [02:29<00:00, 74.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0.01: [{2: array([1.6294335, 1.6242994], dtype=float32)}, {3: array([1.5968527, 1.593365 ], dtype=float32)}], 0.005: [{2: array([1.6241   , 1.6309572], dtype=float32)}, {3: array([1.6312776, 1.6195055], dtype=float32)}]}\n",
            "{0.01: [{2: array([0.20799997, 0.19999997], dtype=float32)}, {3: array([0.22399999, 0.19999997], dtype=float32)}], 0.005: [{2: array([0.264     , 0.21599996], dtype=float32)}, {3: array([0.23199998, 0.23999995], dtype=float32)}]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import numpy as np                   \n",
        "import tensorflow_datasets as tfds    \n",
        "import tensorflow as tf\n",
        "from jax import grad\n",
        "from jax import vmap \n",
        "from functools import partial\n",
        "from jax import jit \n",
        "from jax.experimental import stax \n",
        "from jax.experimental.stax import Conv, Dense, MaxPool, Relu, Flatten, Softmax, LogSoftmax, AvgPool, BatchNorm\n",
        "import matplotlib.pyplot as plt \n",
        "from jax.experimental import optimizers\n",
        "from jax.tree_util import tree_multimap\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "import urllib3\n",
        "import numpy as onp\n",
        "import argparse\n",
        "from jax.experimental import optimizers\n",
        "from jax.tree_util import tree_multimap\n",
        "from tqdm import tqdm\n",
        "\n",
        "# tfds.core.utils.gcs_utils._is_gcs_disabled = True\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "\n",
        "meta_step_size = 0.001\n",
        "\n",
        "batch_size = 25\n",
        "\n",
        "meta_iters = 2000\n",
        "eval_iters = 5\n",
        "inner_iters = 4\n",
        "\n",
        "eval_interval = 1\n",
        "train_shots = 20\n",
        "shots = 5\n",
        "classes = 5\n",
        "\n",
        "class Dataset:\n",
        "    # This class will facilitate the creation of a few-shot dataset\n",
        "    # from the Omniglot dataset that can be sampled from quickly while also\n",
        "    # allowing to create new labels at the same time.\n",
        "    def __init__(self, training):\n",
        "        # Download the tfrecord files containing the omniglot data and convert to a\n",
        "        # dataset.\n",
        "        split = \"train\" if training else \"test\"\n",
        "        ds = tfds.load(\"omniglot\", split=split, as_supervised=True, shuffle_files=False)\n",
        "        # Iterate over the dataset to get each individual image and its class,\n",
        "        # and put that data into a dictionary.\n",
        "        self.data = {}\n",
        "\n",
        "        def extraction(image, label):\n",
        "            # This function will shrink the Omniglot images to the desired size,\n",
        "            # scale pixel values and convert the RGB image to grayscale\n",
        "            image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "            image = tf.image.rgb_to_grayscale(image)\n",
        "            image = tf.image.resize(image, [28, 28])\n",
        "            return image, label\n",
        "\n",
        "        for image, label in ds.map(extraction):\n",
        "            image = image.numpy()\n",
        "            label = str(label.numpy())\n",
        "            if label not in self.data:\n",
        "                self.data[label] = []\n",
        "            self.data[label].append(image)\n",
        "        self.labels = list(self.data.keys())\n",
        "\n",
        "\n",
        "    def get_mini_dataset(\n",
        "        self, shots = shots, num_classes = classes\n",
        "    ):\n",
        "        temp_labels = np.zeros(shape=(num_classes * shots))\n",
        "        temp_images = np.zeros(shape=(num_classes * shots, 28, 28, 1))\n",
        "        test_labels = np.zeros(shape=(num_classes))\n",
        "        test_images = np.zeros(shape=(num_classes, 28, 28, 1))\n",
        "\n",
        "        # Get a random subset of labels from the entire label set.\n",
        "        label_subset = random.choices(self.labels, k=num_classes)\n",
        "        for class_idx, class_obj in enumerate(label_subset):\n",
        "            # Use enumerated index value as a temporary label for mini-batch in\n",
        "            # few shot learning.\n",
        "            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n",
        "            # label to create the test dataset.\n",
        "            test_labels[class_idx] = class_idx\n",
        "            images_to_split = random.choices(\n",
        "                self.data[label_subset[class_idx]], k=shots + 1\n",
        "            )\n",
        "            test_images[class_idx] = images_to_split[-1]\n",
        "            temp_images[\n",
        "                class_idx * shots : (class_idx + 1) * shots\n",
        "            ] = images_to_split[:-1]\n",
        "        \n",
        "        temp_images, temp_labels = shuffle(temp_images.astype(np.float32), temp_labels.astype(np.int32))\n",
        "        \n",
        "        support_set = {'images': temp_images, 'labels': temp_labels}\n",
        "        query_set = {'images': test_images, 'labels': test_labels}\n",
        "        \n",
        "        return support_set, query_set\n",
        "\n",
        "urllib3.disable_warnings()  # Disable SSL warnings that may happen during download.\n",
        "\n",
        "losses = {}\n",
        "accuracies = {}\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"--inner-loops\", help=\"number of inner loops: 1 2 3 4\", type=int, nargs=\"+\", default=1\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--learning-rates\", help=\"learning rates: 0.01 0.05 0.001\", type=float, nargs=\"+\", default=0.01\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--train-steps\", help=\"number of training steps: 100\", type=int, default=100\n",
        ")\n",
        "\n",
        "# args = parser.parse_args()\n",
        "# learning_rates = args.learning_rates\n",
        "# num_inner_loops = args.inner_loops\n",
        "# TRAIN_STEPS = args.train_steps\n",
        "learning_rates = [0.01,0.005]\n",
        "num_inner_loops = [2,3]\n",
        "TRAIN_STEPS = 2\n",
        "\n",
        "# write the accuracy and losses to these files\n",
        "loss_file = open(\"losses.txt\", \"w\")\n",
        "accuracy_file = open(\"accuracy.txt\", \"w\")\n",
        "\n",
        "for lr in learning_rates:\n",
        "  for inner_loop in num_inner_loops:\n",
        "    maml_losses = []\n",
        "    maml_accuracy = []\n",
        "    for i in tqdm(range(TRAIN_STEPS)):\n",
        "      train_dataset = Dataset(training=True)\n",
        "      test_dataset = Dataset(training=False)\n",
        "\n",
        "      net_init, net_apply = stax.serial(\n",
        "          Conv(out_chan = 64, filter_shape = (3,3), strides = [1,1], padding = 'SAME'), BatchNorm(), Relu,\n",
        "          Conv(out_chan = 64, filter_shape = (3,3), strides = [1,1], padding = 'SAME'), BatchNorm(), Relu,\n",
        "          Conv(out_chan = 64, filter_shape = (3,3), strides = [1,1], padding = 'SAME'), BatchNorm(), Relu,\n",
        "          Conv(out_chan = 64, filter_shape = (3,3), strides = [1,1], padding = 'SAME'), BatchNorm(), Relu,\n",
        "          AvgPool((28, 28)),\n",
        "          Flatten,\n",
        "          Dense(classes),\n",
        "      )\n",
        "\n",
        "      in_shape = (-1, 28, 28, 1)\n",
        "      out_shape, net_params = net_init(rng, in_shape)\n",
        "\n",
        "      opt_init, opt_update, get_params = optimizers.adam(step_size=meta_step_size)\n",
        "      opt_state = opt_init(net_params)\n",
        "\n",
        "      @jit\n",
        "      def maml_loss_batch(params, x1_b, y1_b, x2_b, y2_b):\n",
        "          '''\n",
        "          input:\n",
        "          - params\n",
        "          - x1_b, y1_b, x2_b, y2_b: batches of sample task \n",
        "          output:\n",
        "          - combined loss of the batch\n",
        "          '''   \n",
        "          return  onp.mean(vmap(partial(maml_loss, params))(x1_b, y1_b, x2_b, y2_b))\n",
        "\n",
        "      @jit\n",
        "      def batch_step(i, opt_state, x1_b, y1_b, x2_b, y2_b):\n",
        "          p = get_params(opt_state)\n",
        "          g = grad(maml_loss_batch)(p, x1_b, y1_b, x2_b, y2_b)\n",
        "          l = maml_loss_batch(p, x1_b, y1_b, x2_b, y2_b)\n",
        "          return opt_update(i, g, opt_state), l\n",
        "\n",
        "\n",
        "      @jit\n",
        "      def loss(params, inputs, targets):\n",
        "          predictions = net_apply(params, inputs)\n",
        "          loss_ = jnp.mean(optax.softmax_cross_entropy(predictions, jax.nn.one_hot(targets, num_classes=classes)))\n",
        "          return loss_\n",
        "\n",
        "      @jit\n",
        "      def inner_update(params, inputs, outputs, alpha = lr):\n",
        "          '''\n",
        "          input:\n",
        "          - params: model's parameters\n",
        "          - inputs\n",
        "          - targets: true label\n",
        "          output\n",
        "          - updated parameters\n",
        "          '''\n",
        "          grads = grad(loss)(params, inputs, outputs)\n",
        "          grad_update_fn = lambda g, state: (state - alpha * g)\n",
        "          return tree_multimap(grad_update_fn, grads, params)\n",
        "\n",
        "      @jit\n",
        "      def maml_loss(params, support_img, support_lab, query_img, query_lab, num_inner_loops=inner_loop):\n",
        "          '''\n",
        "          input:\n",
        "          - params: model's parameters\n",
        "          - x1, y1: task's train set\n",
        "          - x2, y2: task's test set\n",
        "          output:\n",
        "          - Loss after update parameters 1 time on the test set.\n",
        "          '''\n",
        "          params_updated = params\n",
        "          for _ in range(num_inner_loops):\n",
        "              params_updated = inner_update(params_updated, support_img, support_lab)\n",
        "          total_loss = loss(params_updated, query_img, query_lab)\n",
        "          return total_loss\n",
        "\n",
        "      @jit\n",
        "      def acc_func(params, inputs, targets):\n",
        "          predictions = net_apply(params, inputs)\n",
        "          max_pred = jnp.argmax(predictions, -1)\n",
        "          accuracy = jnp.mean(max_pred == targets)\n",
        "          return accuracy\n",
        "\n",
        "      # get x_support, y_support, x_query, y_query batch\n",
        "      x_support_batch = []\n",
        "      y_support_batch = []\n",
        "      x_query_batch = []\n",
        "      y_query_batch = []\n",
        "      for j in range(batch_size):\n",
        "          support, query = train_dataset.get_mini_dataset()\n",
        "          x_support, y_support = support['images'], support['labels']\n",
        "          x_query, y_query = query['images'], query['labels']\n",
        "          x_support_batch.append(x_support)\n",
        "          y_support_batch.append(y_support)\n",
        "          x_query_batch.append(x_query)\n",
        "          y_query_batch.append(y_query)\n",
        "      x_support_batch = np.stack(x_support_batch)\n",
        "      y_support_batch = np.stack(y_support_batch)\n",
        "      x_query_batch = np.stack(x_query_batch)\n",
        "      y_query_batch = np.stack(y_query_batch)\n",
        "      opt_state, l = batch_step(i, opt_state, x_support_batch, y_support_batch, x_query_batch, y_query_batch)\n",
        "      p = get_params(opt_state)\n",
        "      acc_val =  onp.mean(vmap(partial(acc_func, p))(x_query_batch, y_query_batch))\n",
        "      maml_losses.append(l)\n",
        "      maml_accuracy.append(acc_val)\n",
        "\n",
        "    net_params = get_params(opt_state)\n",
        "\n",
        "    loss_file.write(str(lr)+\"  \"+str(inner_loop)+\"  \"+str(np.array(maml_losses))+\"\\n\")\n",
        "    accuracy_file.write(str(lr)+\"  \"+str(inner_loop)+\"  \"+str(np.array(maml_accuracy))+\"\\n\")\n",
        "\n",
        "    if lr in losses.keys():\n",
        "      losses[lr].append({inner_loop: np.array(maml_losses)})\n",
        "      accuracies[lr].append({inner_loop: np.array(maml_accuracy)})\n",
        "    else:\n",
        "      losses[lr] = [{inner_loop: np.array(maml_losses)}]\n",
        "      accuracies[lr] = [{inner_loop: np.array(maml_accuracy)}]\n",
        "\n",
        "print(losses)\n",
        "print(accuracies)\n",
        "\n",
        "loss_file.close()\n",
        "accuracy_file.close()"
      ]
    }
  ]
}